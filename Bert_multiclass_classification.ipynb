{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNt1ri7gdd1RLQXrkcnLzgt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhishekjha111/AI-codes/blob/master/Bert_multiclass_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCymjGUZSL7P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your text data and labels\n",
        "text_data = [...]  # List of customer conversation text\n",
        "labels = [...]  # List of corresponding labels (e.g., \"need improvement,\" \"no improvement,\" etc.)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(set(labels)))\n",
        "\n",
        "# Tokenize the text data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], torch.tensor(train_labels))\n",
        "test_dataset = TensorDataset(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"], torch.tensor(test_labels))\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "        for batch in train_loader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                        optimizer.zero_grad()\n",
        "                                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                                        loss = outputs.loss\n",
        "                                                loss.backward()\n",
        "                                                        optimizer.step()\n",
        "\n",
        "                                                        # Evaluation\n",
        "                                                        model.eval()\n",
        "                                                        correct, total = 0, 0\n",
        "                                                        with torch.no_grad():\n",
        "                                                            for batch in test_loader:\n",
        "                                                                    input_ids, attention_mask, labels = batch\n",
        "                                                                            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                                                                                    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "                                                                                            total += labels.size(0)\n",
        "                                                                                                    correct += (predictions == labels).sum().item()\n",
        "\n",
        "                                                                                                    accuracy = correct / total\n",
        "                                                                                                    print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wMialjz-Sak4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have limited data, which is often the case for many real-world applications, fine-tuning a BERT model can be challenging since BERT models are known to perform best with large datasets. However, there are strategies and techniques you can employ to make the most out of your limited data:\n",
        "\n",
        "Data Augmentation: You can apply data augmentation techniques to artificially increase your training data. For text data, this might involve techniques like synonym replacement, word shuffling, or back-translation. Data augmentation can help diversify your dataset and improve model generalization.\n",
        "\n",
        "Transfer Learning: Instead of fine-tuning a BERT model from scratch, you can start with a pre-trained BERT model and perform \"feature extraction.\" This involves using the pre-trained BERT model to extract contextual word embeddings from your text data. You can then build a smaller classification model on top of these embeddings.\n",
        "\n",
        "Few-Shot Learning: If your data is extremely limited, consider few-shot learning techniques. These methods aim to train models that can perform well on tasks with very few examples. You can use techniques like meta-learning, transfer learning from other related tasks, or pre-trained language models with few-shot capabilities.\n",
        "\n",
        "Regularization: Use regularization techniques such as dropout and weight decay to prevent overfitting, which is a common concern when working with limited data.\n",
        "\n",
        "Semi-Supervised Learning: If you have a small amount of labeled data and a larger amount of unlabeled data, you can explore semi-supervised learning approaches. This involves using a combination of labeled and unlabeled data to train your model.\n",
        "\n",
        "Ensemble Learning: Combine predictions from multiple models to improve classification accuracy. Ensemble techniques can help compensate for the limited data.\n",
        "\n",
        "Domain-Specific Pretraining: If your task is domain-specific, you can explore domain-specific pretraining. This involves training BERT on a larger dataset from your specific domain before fine-tuning on your limited data.\n",
        "\n",
        "Active Learning: Start with a small labeled dataset and iteratively select and label examples that the model is uncertain about. This approach allows you to maximize the value of each labeled example.\n",
        "\n",
        "Domain Knowledge: Incorporate domain knowledge into your model to provide additional information that might be lacking in the limited data.\n",
        "\n",
        "Evaluate Carefully: When working with limited data, it's crucial to use appropriate evaluation metrics and cross-validation techniques to ensure you're getting an accurate assessment of your model's performance.\n",
        "\n",
        "Remember that even with limited data, you can still build useful models, but it might require more experimentation and careful consideration of the techniques that best suit your specific problem.\n"
      ],
      "metadata": {
        "id": "GvzYmMkESocp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main code"
      ],
      "metadata": {
        "id": "QSexMLdbUJfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JBvRCUfHULtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code fine-tunes BERT for multiclass classification of text data.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the text data and labels\n",
        "text_data = [...]  # List of customer conversation text\n",
        "labels = [...]  # List of corresponding labels (e.g., \"need improvement,\" \"no improvement,\" etc.)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(text_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(set(labels)))\n",
        "\n",
        "# Tokenize the text data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_dataset = TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"], torch.tensor(train_labels))\n",
        "test_dataset = TensorDataset(test_encodings[\"input_ids\"], test_encodings[\"attention_mask\"], torch.tensor(test_labels))\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "        for batch in train_loader:\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                        optimizer.zero_grad()\n",
        "                                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                                        loss = outputs.loss\n",
        "                                                loss.backward()\n",
        "                                                        optimizer.step()\n",
        "\n",
        "                                                        # Evaluation\n",
        "                                                        model.eval()\n",
        "                                                        correct, total = 0, 0\n",
        "                                                        with torch.no_grad():\n",
        "                                                            for batch in test_loader:\n",
        "                                                                    input_ids, attention_mask, labels = batch\n",
        "                                                                            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                                                                                    predictions = torch.argmax(outputs.logits, dim=1)\n",
        "                                                                                            total += labels.size(0)\n",
        "                                                                                                    correct += (predictions == labels).sum().item()\n",
        "\n",
        "                                                                                                    accuracy = correct / total\n",
        "                                                                                                    print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
        "\n",
        "                                                                                                    # Save the fine-tuned model\n",
        "                                                                                                    torch.save(model.state_dict(), 'fine-tuned-bert-speech-classification-model.pt')"
      ],
      "metadata": {
        "id": "QB3qRguESuez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code you provided, text_data and labels are represented as Python lists. If you have your text data and labels in a different format, such as a CSV file or other data storage options, you'll need to read the data from your chosen format and convert it into Python lists.\n",
        "\n",
        "Here's an example of how to load text data and labels from a CSV file using the pandas library and convert them into Python lists:"
      ],
      "metadata": {
        "id": "IUJJISyIUfpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data from a CSV file\n",
        "data = pd.read_csv('your_data.csv')\n",
        "\n",
        "# Extract text data and labels\n",
        "text_data = data['text_column'].tolist()  # 'text_column' should be replaced with the actual column name containing your text data\n",
        "labels = data['label_column'].tolist()  # 'label_column' should be replaced with the actual column name containing your labels\n"
      ],
      "metadata": {
        "id": "I5h-FJU3Ujsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace 'your_data.csv', 'text_column', and 'label_column' with the appropriate file path and column names in your dataset.\n",
        "\n",
        "If your data is stored in a different format or location, you may need to use different libraries or methods for data loading and manipulation. The key is to ensure that text_data and labels are Python lists, where text_data contains the text of your customer conversations, and labels contains the corresponding labels for those conversations.\n",
        "\n",
        "After loading and preparing your data, you can use these Python lists in the provided code for fine-tuning the BERT model as described earlier."
      ],
      "metadata": {
        "id": "yp6K4etjUn4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For larger datasets, you can work with the data directly as in-memory Python lists, as shown in the code example. Loading the data into memory from a storage format like a CSV file is a common and practical approach for moderately-sized datasets.\n",
        "\n",
        "However, when you're dealing with very large datasets that may not fit into memory or if you want to take advantage of distributed computing and efficient data access, you can consider using data storage options like:\n",
        "\n",
        "Data Databases: Storing your data in a database, such as SQL or NoSQL databases, can help manage and access large volumes of data efficiently. You can use database libraries to retrieve batches of data as needed for training.\n",
        "\n",
        "Data Lakes: Data lakes, often built on top of distributed file systems like Hadoop HDFS or cloud-based solutions like Amazon S3, are suitable for storing large datasets in various formats (e.g., Parquet, Avro). Tools like Apache Spark can be used to process and read data from data lakes efficiently.\n",
        "\n",
        "Data Generators: For very large datasets that don't fit into memory, you can use data generators in PyTorch or TensorFlow. These generators allow you to load and preprocess data on-the-fly in smaller batches, which is especially useful for deep learning models.\n",
        "\n",
        "Cloud Storage: If you're working with data stored in the cloud (e.g., AWS S3, Google Cloud Storage), you can use cloud SDKs to access data directly without needing to load the entire dataset into memory.\n",
        "\n",
        "Whether you should use a storage option like a data lake or a cloud-based approach often depends on the scale of your data and the infrastructure available to you. It's a trade-off between the convenience of in-memory processing and the scalability and efficiency of external storage solutions.\n",
        "\n",
        "When working with large datasets, you'll also need to consider data streaming and batching techniques to train your model in smaller increments. This can help you avoid out-of-memory errors and improve training efficiency"
      ],
      "metadata": {
        "id": "7B5wNNqvU0aR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pv9MJX2SU3ep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}