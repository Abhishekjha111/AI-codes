{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahjnffIakRVM",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiBFCTrEYUPd",
        "colab_type": "text"
      },
      "source": [
        "According to industry estimates, only 21% of the available data is present in structured form. Data is being generated as we speak, as we tweet, as we send messages on Whatsapp and in various other activities. Majority of this data exists in the textual form, which is highly unstructured in nature.\n",
        "\n",
        "Few notorious examples include – tweets / posts on social media, user to user chat conversations, news, blogs and articles, product or services reviews and patient records in the healthcare sector. A few more recent ones includes chatbots and other voice driven bots.\n",
        "\n",
        "Despite having high dimension data, the information present in it is not directly accessible unless it is processed (read and understood) manually or analyzed by an automated system. In order to produce significant and actionable insights from text data, it is important to get acquainted with the techniques and principles of Natural Language Processing (NLP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LImnkZFUXIJQ",
        "colab_type": "code",
        "outputId": "35258460-7998-4605-f26a-7c21dfb4982b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SszwU6qycH2Y",
        "colab_type": "code",
        "outputId": "5ca758e5-96ad-4c12-d92c-70408f7e140e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk  \n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: q\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIrwMMnWlAD0",
        "colab_type": "code",
        "outputId": "626d61d8-828e-4310-9d58-2617dab2a46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "! pip install spacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.4)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.16.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.6.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.2)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSzOa6GgX0vM",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Natural Language Processing\n",
        "\n",
        "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner. By utilizing NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as – automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n",
        "\n",
        "Before moving further, lets have a look at few keywords used frequently.\n",
        "\n",
        "Tokenization – process of converting a text into tokens\n",
        "\n",
        "Tokens – words or entities present in the text\n",
        "\n",
        "Text object – a sentence or a phrase or a word or an article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x4eme8NYhFQ",
        "colab_type": "text"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "Since, text is the most unstructured form of all the available data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing.\n",
        "\n",
        "It is predominantly comprised of three steps:\n",
        "\n",
        "* Noise Removal\n",
        "* Lexicon Normalization\n",
        "* Object Standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Sf5tN1YpUJ",
        "colab_type": "text"
      },
      "source": [
        "### Noise Removal\n",
        "\n",
        "Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n",
        "\n",
        "For example – language stopwords (commonly used words of a language – is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text. A general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh7flJfMZB3b",
        "colab_type": "code",
        "outputId": "d0cea3c8-9bb6-4f25-a132-b7e797cfe51b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Sample code to remove noisy words from a text\n",
        "\n",
        "noise_list = [\"is\", \"a\", \"this\", \"...\",'to', 'and'] \n",
        "def remove_noise(input_text):\n",
        "    words = input_text.split() \n",
        "    noise_free_words = [word for word in words if word not in noise_list] \n",
        "    noise_free_text = \" \".join(noise_free_words) \n",
        "    return noise_free_text\n",
        "\n",
        "remove_noise(\"this is a sample text and i'll go to market now\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"sample text i'll go market now\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJOBXZ8KuRC8",
        "colab_type": "code",
        "outputId": "de8b9477-7611-4e2a-8ef3-98108e42fa16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVIYVfTSt891",
        "colab_type": "code",
        "outputId": "ac383adf-0ca1-4638-b81c-341af8d884c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTL5aYaBZOzA",
        "colab_type": "text"
      },
      "source": [
        "Another approach is to use the regular expressions while dealing with special patterns of noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5-IsikHZPZm",
        "colab_type": "code",
        "outputId": "a3dd05bc-9482-4219-eb48-b9a1ac0bf91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Sample code to remove a regex pattern \n",
        "import re \n",
        "\n",
        "def remove_regex(input_text, regex_pattern):\n",
        "    urls = re.finditer(regex_pattern, input_text) \n",
        "    for i in urls: \n",
        "        input_text = re.sub(i.group().strip(), '', input_text)\n",
        "    return input_text\n",
        "\n",
        "regex_pattern = \"#[\\w]*\"  \n",
        "\n",
        "remove_regex(\"remove this #hashtag from my given string object\", regex_pattern)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'remove this  from my given string object'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhDUTe23ZkHq",
        "colab_type": "text"
      },
      "source": [
        "**Lexicon Normalization**\n",
        "\n",
        "Another type of textual noise is about the multiple representations exhibited by single word.\n",
        "\n",
        "For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma). Normalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.\n",
        "\n",
        "The most common lexicon normalization practices are :\n",
        "\n",
        "* Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
        "* Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPjG2DUDaNUQ",
        "colab_type": "code",
        "outputId": "51f21b83-7bc7-4503-e72f-c40058e85015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvy7CGwMZzk-",
        "colab_type": "code",
        "outputId": "14dba0e9-5c3f-460c-e650-d154dd0aac34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "stem = PorterStemmer()\n",
        "\n",
        "word = \"Standardization\" \n",
        "\n",
        "print(lem.lemmatize(word, \"v\"))\n",
        "print(stem.stem(word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardization\n",
            "standard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA46juLHakJ0",
        "colab_type": "text"
      },
      "source": [
        "**Object Standardization**\n",
        "\n",
        "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
        "\n",
        "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voFNHJg_apP2",
        "colab_type": "code",
        "outputId": "3c1dd4c5-b73e-482e-8355-c7148c2c27ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
        "def lookup_words(input_text):\n",
        "    words = input_text.split() \n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "        if word.lower() in lookup_dict:\n",
        "            word = lookup_dict[word.lower()]\n",
        "        new_words.append(word) \n",
        "        new_text = \" \".join(new_words) \n",
        "        return new_text\n",
        "\n",
        "print(lookup_words(\"RT this is a retweeted tweet by Doland J.Trump\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Retweet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q21u1JbIcfjC",
        "colab_type": "text"
      },
      "source": [
        "### Text to Features (Feature Engineering on text data)\n",
        "To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Syntactical Parsing, Entities / N-grams / word-based features, Statistical features, and word embeddings. Read on to understand these techniques in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjXewXpych5w",
        "colab_type": "text"
      },
      "source": [
        "**Syntactic Parsing**\n",
        "\n",
        "Syntactical parsing involves the analysis of words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words. Dependency Grammar and Part of Speech tags are the important attributes of text syntactics.\n",
        "\n",
        "* Dependency Trees – Sentences are composed of some words sewed together. The relationship among the words in a sentence is determined by the basic dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words). Every relation can be represented in the form of a triplet (relation, governor, dependent). For example: consider the sentence – “**Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas**.” The relationship among the words can be observed in the form of a tree representation as shown\n",
        "\n",
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11181146/image-2.png)\n",
        "\n",
        "\n",
        "The tree shows that “submitted” is the root word of this sentence, and is linked by two sub-trees (subject and object subtrees). Each subtree is a itself a dependency tree with relations such as – (“Bills” <-> “ports” <by> “proposition” relation), (“ports” <-> “immigration” <by> “conjugation” relation).\n",
        "\n",
        "This type of tree, when parsed recursively in top-down manner gives grammar relation triplets as output which can be used as features for many nlp problems like entity wise sentiment analysis, actor & entity identification, and text classification. The python wrapper StanfordCoreNLP (by Stanford NLP Group, only commercial license) and NLTK dependency grammars can be used to generate dependency trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqzeviVPdMl2",
        "colab_type": "text"
      },
      "source": [
        "**Part of speech tagging** – Apart from the grammar relations, every word in a sentence is also associated with a part of speech tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. Here is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (it provides several implementations, the default one is perceptron tagger)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjXdvR63duQy",
        "colab_type": "code",
        "outputId": "ab538ab8-3394-41da-c7dc-b90963228686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7nA04gvd9mp",
        "colab_type": "code",
        "outputId": "e030d65b-7377-4af7-cc38-7d4c1062a73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEFNCgKLdfuY",
        "colab_type": "code",
        "outputId": "559f7d53-6147-41b1-d45c-d249c5fb45bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "text = \"I am going to travel the world and click lot of beautiful pictures\"\n",
        "tokens = word_tokenize(text)\n",
        "print (pos_tag(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('travel', 'VB'), ('the', 'DT'), ('world', 'NN'), ('and', 'CC'), ('click', 'NN'), ('lot', 'NN'), ('of', 'IN'), ('beautiful', 'JJ'), ('pictures', 'NNS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV8beJSIfTXb",
        "colab_type": "text"
      },
      "source": [
        "**A.Word sense disambiguation**: Some language words have multiple meanings according to their usage. For example, in the two sentences below:\n",
        "\n",
        "I. “Please book my flight for Delhi”\n",
        "\n",
        "II. “I am going to read this book in the flight”\n",
        "\n",
        "“Book” is used with different context, however the part of speech tag for both of the cases are different. In sentence I, the word “book” is used as verb, while in II it is used as noun. (Lesk Algorithm is also us ed for similar purposes)\n",
        "\n",
        "**B.Improving word-based features**: A learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:\n",
        "\n",
        "Sentence -“book my flight, I will read this book”\n",
        "\n",
        "Tokens – (“book”, 2), (“my”, 1), (“flight”, 1), (“I”, 1), (“will”, 1), (“read”, 1), (“this”, 1)\n",
        "\n",
        "Tokens with POS – (“book_VB”, 1), (“my_PRP$”, 1), (“flight_NN”, 1), (“I_PRP”, 1), (“will_MD”, 1), (“read_VB”, 1), (“this_DT”, 1), (“book_NN”, 1)\n",
        "\n",
        "**C.Normalization and Lemmatization**: POS tags are the basis of lemmatization process for converting a word to its base form (lemma).\n",
        "\n",
        "**D.Efficient stopword removal** : POS tags are also useful in efficient removal of stopwords.\n",
        "\n",
        "For example, there are some tags which always define the low frequency / less important words of a language. For example: (IN – “within”, “upon”, “except”), (CD – “one”,”two”, “hundred”), (MD – “may”, “mu st” etc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0o3LoA7fgCq",
        "colab_type": "text"
      },
      "source": [
        "### Entity Extraction (Entities as features)\n",
        "Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both. Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.\n",
        "\n",
        "\n",
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11181407/image-3.png)\n",
        "\n",
        "**A. Named Entity Recognition (NER)**\n",
        "\n",
        "The process of detecting the named entities such as person names, location names, company names etc from the text is called as NER. For example :\n",
        "\n",
        "Sentence – Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\n",
        "\n",
        "Named Entities –  ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)\n",
        "\n",
        "A typical NER model consists of three blocks:\n",
        "\n",
        "* Noun phrase identification: This step deals with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.\n",
        "\n",
        "* Phrase classification: This is the classification step in which all the extracted noun phrases are classified into respective categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate the lookup tables and dictionaries by combining information from different sources.\n",
        "\n",
        "* Entity disambiguation: Sometimes it is possible that entities are misclassified, hence creating a validation layer on top of the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are – Google Knowledge Graph, IBM Watson and Wikipedia. \n",
        "\n",
        " \n",
        "\n",
        "**B. Topic Modeling**\n",
        "\n",
        "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PFTvIN4iods",
        "colab_type": "text"
      },
      "source": [
        "Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts.\n",
        "\n",
        "Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
        "\n",
        "Topic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example – New York Times are using topic models to boost their user – article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.\n",
        "\n",
        "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G8gfpTthXMd",
        "colab_type": "code",
        "outputId": "f21186d7-17ae-4df7-c88e-0ee4efb93daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "! pip install gensim\n",
        "! pip install corpora"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.4)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.16.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.175)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.175 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.175)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.175->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.175->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "Collecting corpora\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/f5/998ee3d19c64e42a5a3839858ede61ccd504c13f24fbe3bf48ddb6fd3592/Corpora-1.0.tar.gz\n",
            "Building wheels for collected packages: corpora\n",
            "  Building wheel for corpora (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/05/3f/335a581f1159688e17600c6363af906dbdb52515faf43dd448\n",
            "Successfully built corpora\n",
            "Installing collected packages: corpora\n",
            "Successfully installed corpora-1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3exoO92qhy1s",
        "colab_type": "code",
        "outputId": "ccd0d773-4f67-4e0b-bc35-1a01a1d2dee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "! pip install corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting corpus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/b9/120d9e0ae8702a6929946b494b723a4de6c9bf3d79e8e07e239a81be4e7c/Corpus-0.4.2.tar.gz (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: corpus\n",
            "  Building wheel for corpus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/20/6d/214e9c84ce43f62538d4c2f6e23d412bf9a52dd0f12bc716c9\n",
            "Successfully built corpus\n",
            "Installing collected packages: corpus\n",
            "Successfully installed corpus-0.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTXF4Vyhi6kI",
        "colab_type": "text"
      },
      "source": [
        "**Latent Dirichlet Allocation for Topic Modeling**\n",
        "\n",
        "There are many approaches for obtaining topics from a text such as – Term Frequency and Inverse Document Frequency. NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation is the most popular topic modeling technique and in this article, we will discuss the same.\n",
        "\n",
        "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.\n",
        "\n",
        "LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. The following matrix shows a corpus of N documents D1, D2, D3 … Dn and vocabulary size of M words W1,W2 .. Wn. The value of i,j cell gives the frequency count of word Wj in Document Di.\n",
        "\n",
        "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling2.png)\n",
        "\n",
        "LDA converts this Document-Term Matrix into two lower dimensional matrices – M1 and M2.\n",
        "M1 is a document-topics matrix and M2 is a topic – terms matrix with dimensions (N,  K) and (K, M) respectively, where N is the number of documents, K is the number of topics and M is the vocabulary size.\n",
        "\n",
        "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/modeling3.png)\n",
        "\n",
        "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling4.png)\n",
        "\n",
        "otice that these two matrices already provides topic word and document topic distributions, However, these distribution needs to be improved, which is the main aim of LDA. LDA makes use of sampling techniques in order to improve these matrices.\n",
        "\n",
        "It Iterates through each word “w” for each document “d” and tries to adjust the current topic – word assignment with a new assignment. A new topic “k” is assigned to word “w” with a probability P which is a product of two probabilities p1 and p2.\n",
        "\n",
        "For every topic, two probabilities p1 and p2 are calculated. P1 – p(topic t / document d) = the proportion of words in document d that are currently assigned to topic t. P2 – p(word w / topic t) = the proportion of assignments to topic t over all documents that come from this word w.\n",
        "\n",
        "The current topic – word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word – topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current word’s topic with new probability.\n",
        "\n",
        "After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixe5QmIIhJmK",
        "colab_type": "code",
        "outputId": "e2d396a9-44a7-4fe1-af1d-9448439578f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
        "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
        "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
        "doc_complete = [doc1, doc2, doc3]\n",
        "doc_clean = [doc.split() for doc in doc_complete]\n",
        "\n",
        "import gensim\n",
        "# import corpora\n",
        "\n",
        "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
        "dictionary = gensim.corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Training LDA model on the document term matrix\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
        "\n",
        "# Results \n",
        "print(ldamodel.print_topics())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.064*\"driving\" + 0.037*\"dance\" + 0.037*\"father\" + 0.037*\"of\" + 0.037*\"around\" + 0.037*\"a\" + 0.037*\"practice.\" + 0.037*\"spends\" + 0.037*\"time\" + 0.037*\"lot\"'), (1, '0.029*\"My\" + 0.029*\"sister\" + 0.029*\"my\" + 0.029*\"to\" + 0.029*\"and\" + 0.029*\"blood\" + 0.029*\"increased\" + 0.029*\"cause\" + 0.029*\"suggest\" + 0.029*\"stress\"'), (2, '0.089*\"to\" + 0.051*\"My\" + 0.051*\"sister\" + 0.051*\"my\" + 0.051*\"is\" + 0.051*\"sugar,\" + 0.051*\"not\" + 0.051*\"Sugar\" + 0.051*\"consume.\" + 0.051*\"but\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iXpkFC0pRph",
        "colab_type": "text"
      },
      "source": [
        "### Statistical Features\n",
        "\n",
        "Text data can also be quantified directly into numbers using several techniques described in this section:\n",
        "\n",
        "**Term Frequency – Inverse Document Frequency (TF – IDF)**\n",
        "\n",
        "TF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example – let say there is a dataset of N text documents, In any document “D”, TF and IDF will be defined as –\n",
        "\n",
        "Term Frequency (TF) – TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
        "\n",
        "Inverse Document Frequency (IDF) – IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T.\n",
        "\n",
        "TF . IDF – TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula below. Following is the code using python’s scikit learn package to convert a text into tf idf vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yfrTcjKpgIK",
        "colab_type": "code",
        "outputId": "ca879b62-de1e-489e-bf89-30af489b082a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "obj = TfidfVectorizer()\n",
        "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
        "X = obj.fit_transform(corpus)\n",
        "print (X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t0.34520501686496574\n",
            "  (0, 4)\t0.444514311537431\n",
            "  (0, 2)\t0.5844829010200651\n",
            "  (0, 7)\t0.5844829010200651\n",
            "  (1, 3)\t0.652490884512534\n",
            "  (1, 0)\t0.652490884512534\n",
            "  (1, 1)\t0.3853716274664007\n",
            "  (2, 5)\t0.5844829010200651\n",
            "  (2, 6)\t0.5844829010200651\n",
            "  (2, 1)\t0.34520501686496574\n",
            "  (2, 4)\t0.444514311537431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC9iHXiRpu-N",
        "colab_type": "text"
      },
      "source": [
        "The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj8xhJU0qnN8",
        "colab_type": "text"
      },
      "source": [
        "### Word Embedding (text vectors)\n",
        "Word embedding is the modern way of representing words as vectors. The aim of word embedding is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are widely used in deep learning models such as Convolutional Neural Networks and Recurrent Neural Networks.\n",
        "\n",
        "Word2Vec and GloVe are the two popular models to create word embedding of a text. These models takes a text corpus as input and produces the word vectors as output.\n",
        "\n",
        "Word2Vec model is composed of preprocessing module, a shallow neural network model called Continuous Bag of Words and another shallow neural network model called skip-gram. These models are widely used for all other nlp problems. It first constructs a vocabulary from the training corpus and then learns word embedding representations. Following code using gensim package prepares the word embedding as the vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdUS1XF1q2Ec",
        "colab_type": "code",
        "outputId": "933da6aa-8328-4770-a11b-02c89ef433a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics', 'physics'],['machine', 'learning'], ['deep', 'learning']]\n",
        "\n",
        "# train the model on your corpus  \n",
        "model = Word2Vec(sentences, min_count = 1)\n",
        "\n",
        "print (model.similarity('physics', 'science'))\n",
        "print (model['learning'] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.033712905\n",
            "[-8.0814573e-04 -8.7587448e-04  5.1021902e-04  9.3901437e-04\n",
            " -7.7165035e-04 -4.4212416e-03  1.3058307e-03 -1.3669888e-03\n",
            "  2.7214405e-03  2.1084184e-03 -1.9986353e-04 -2.2760585e-03\n",
            " -2.8680824e-03  3.7468798e-03 -4.1943663e-03 -8.5004524e-04\n",
            " -3.1149634e-03  3.6883876e-03 -4.2184005e-03 -1.0536205e-03\n",
            "  3.1640546e-03 -2.5055527e-03 -3.6515580e-03 -4.6548708e-03\n",
            " -2.7767889e-04  1.1244148e-03 -5.6641124e-04  7.4233179e-04\n",
            " -2.6709610e-03  6.2946306e-04  4.1819904e-03  2.6962438e-04\n",
            "  4.9294308e-03  4.0743388e-03 -2.1657487e-03 -4.1134851e-03\n",
            "  3.3338380e-03  1.5682331e-03  1.2704531e-03 -2.4027969e-03\n",
            " -3.6331662e-03  2.3212475e-03 -3.7614624e-03  4.3548444e-05\n",
            " -4.0118000e-03  2.2267654e-04 -5.2493281e-04 -3.9842254e-03\n",
            " -4.8402925e-03  2.3378034e-03 -2.4590113e-03  2.2121710e-03\n",
            " -4.8007150e-03  4.0628114e-03  2.8450009e-03 -4.9105524e-03\n",
            " -3.9016288e-03  1.6618224e-03  5.9539196e-04  4.4487547e-03\n",
            "  1.9059177e-03 -3.3338221e-03 -3.1875139e-03 -3.7504814e-04\n",
            " -4.9301819e-03  1.7957765e-04  1.8469060e-03 -2.2631567e-03\n",
            "  2.3849872e-03  4.2942339e-03  1.9246910e-04 -4.1866549e-03\n",
            " -3.2990812e-03  4.5451927e-03 -8.0321816e-04  2.9306714e-03\n",
            "  3.8280366e-03 -3.8515643e-04  1.4117887e-03 -4.3546999e-04\n",
            " -1.2199049e-03 -7.6238415e-04  1.1372046e-03  4.2644758e-03\n",
            "  1.0399901e-03  4.4126431e-03 -3.6411341e-03  4.9951007e-03\n",
            "  4.1250563e-03  4.0011732e-03 -1.1660283e-03  1.2074428e-03\n",
            " -9.5208286e-04  1.1521940e-03  3.8405752e-03 -2.7363523e-04\n",
            "  3.0312175e-03 -3.9406554e-03  1.4146091e-03  1.0473398e-03]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uErFuaLprF-Z",
        "colab_type": "text"
      },
      "source": [
        "They can be used as feature vectors for ML model, used to measure text similarity using cosine similarity techniques, words clustering and text classification techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUkmlWYkrM7J",
        "colab_type": "text"
      },
      "source": [
        "### Text Classification\n",
        "Text classification is one of the classical problem of NLP. Notorious examples include – Email Spam Identification, topic classification of news, sentiment classification and organization of web pages by search engines.\n",
        "\n",
        "Text classification, in common words is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed category. It is really helpful when the amount of data is too large, especially for organizing, information filtering, and storage purposes.\n",
        "\n",
        "A typical natural language classifier consists of two parts: (a) Training (b) Prediction as shown in image below. Firstly the text input is processes and features are created. The machine learning models then learn these features and is used for predicting against the new text.\n",
        "\n",
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/11182015/image-5.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeRouolPtmtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_corpus = [\n",
        "                   ('I am exhausted of this work.', 'Class_B'),\n",
        "                   (\"I can't cooperate with this\", 'Class_B'),\n",
        "                   ('He is my badest enemy!', 'Class_B'),\n",
        "                   ('My management is poor.', 'Class_B'),\n",
        "                   ('I love this burger.', 'Class_A'),\n",
        "                   ('This is an brilliant place!', 'Class_A'),\n",
        "                   ('I feel very good about these dates.', 'Class_A'),\n",
        "                   ('This is my best work.', 'Class_A'),\n",
        "                   (\"What an awesome view\", 'Class_A'),\n",
        "                   ('I do not like this dish', 'Class_B')]\n",
        "test_corpus = [\n",
        "                (\"I am not feeling well today.\", 'Class_B'), \n",
        "                (\"I feel brilliant!\", 'Class_A'), \n",
        "                ('Gary is a friend of mine.', 'Class_A'), \n",
        "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
        "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9WtTVo_rGxL",
        "colab_type": "code",
        "outputId": "2c3d00dc-e96f-4401-c2d7-2f55b780c8f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "import sklearn.feature_extraction.text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn import svm \n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\n",
        "train_data = []\n",
        "train_labels = []\n",
        "for row in training_corpus:\n",
        "    train_data.append(row[0])\n",
        "    train_labels.append(row[1])\n",
        "\n",
        "test_data = [] \n",
        "test_labels = [] \n",
        "for row in test_corpus:\n",
        "    test_data.append(row[0]) \n",
        "    test_labels.append(row[1])\n",
        "\n",
        "print(train_data)\n",
        "# Create feature vectors \n",
        "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
        "# Train the feature vectors\n",
        "train_vectors = vectorizer.fit_transform(train_data)\n",
        "# Apply model on test data \n",
        "test_vectors = vectorizer.transform(test_data)\n",
        "\n",
        "print(train_vectors)\n",
        "# Perform classification with SVM, kernel=linear \n",
        "model = svm.SVC(kernel='linear') \n",
        "model.fit(train_vectors, train_labels) \n",
        "prediction = model.predict(test_vectors)\n",
        "\n",
        "\n",
        "print (classification_report(test_labels, prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am exhausted of this work.', \"I can't cooperate with this\", 'He is my badest enemy!', 'My management is poor.', 'I love this burger.', 'This is an brilliant place!', 'I feel very good about these dates.', 'This is my best work.', 'What an awesome view', 'I do not like this dish']\n",
            "  (0, 1)\t1.0\n",
            "  (1, 1)\t1.0\n",
            "  (2, 0)\t1.0\n",
            "  (3, 0)\t1.0\n",
            "  (4, 1)\t1.0\n",
            "  (5, 0)\t0.776355388538153\n",
            "  (5, 1)\t0.6302954154107211\n",
            "  (7, 0)\t0.776355388538153\n",
            "  (7, 1)\t0.6302954154107211\n",
            "  (9, 1)\t1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class_A       0.50      0.67      0.57         3\n",
            "     Class_B       0.50      0.33      0.40         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.50      0.50      0.49         6\n",
            "weighted avg       0.50      0.50      0.49         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_ClflSqu9in",
        "colab_type": "text"
      },
      "source": [
        "The text classification model are heavily dependent upon the quality and quantity of features, while applying any machine learning model it is always a good practice to include more and more training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtDVNmV0vE32",
        "colab_type": "text"
      },
      "source": [
        "**NLP problems / tasks**\n",
        "* Text Summarization – Given a text article or paragraph, summarize it automatically to produce most important and relevant sentences in order.\n",
        "* Machine Translation – Automatically translate text from one human language to another by taking care of grammar, semantics and information about the real world, etc.\n",
        "* Natural Language Generation and Understanding – Convert information from computer databases or semantic intents into readable human language is called language generation. Converting chunks of text into more logical structures that are easier for computer programs to manipulate is called language understanding.\n",
        "* Optical Character Recognition – Given an image representing printed text, determine the corresponding text.\n",
        "* Document to Information – This involves parsing of textual data present in documents (websites, files, pdfs and images) to analyzable and clean format.\n",
        "\n",
        " \n",
        "\n",
        "**Important Libraries for NLP (python)**\n",
        "\n",
        "* Scikit-learn: Machine learning in Python\n",
        "* Natural Language Toolkit (NLTK): The complete toolkit for all NLP techniques.\n",
        "* Pattern – A web mining module for the with tools for NLP and machine learning.\n",
        "* TextBlob – Easy to use nlp tools API, built on top of NLTK and Pattern.\n",
        "* spaCy – Industrial strength NLP with Python and Cython.\n",
        "* Gensim – Topic Modelling for Humans\n",
        "* Stanford Core NLP – NLP services and packages by Stanford NLP Group."
      ]
    }
  ]
}