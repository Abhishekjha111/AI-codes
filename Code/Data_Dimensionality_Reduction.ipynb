{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Dimensionality Reduction",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U06Yj8GcpK0o",
        "colab_type": "text"
      },
      "source": [
        "## What is Dimensionality Reduction?\n",
        "\n",
        "We are generating a tremendous amount of data daily. In fact, 90% of the data in the world has been generated in the last 3-4 years! The numbers are truly mind boggling. Below are just some of the examples of the kind of data being collected:\n",
        "\n",
        "* Facebook collects data of what you like, share, post, places you visit, restaurants you like, etc.\n",
        "* Your smartphone apps collect a lot of personal information about you\n",
        "* Amazon collects data of what you buy, view, click, etc. on their site\n",
        "* Casinos keep a track of every move each customer makes\n",
        "\n",
        "As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging. We can easily visualize 2-d data but once dimensions start to increase it become impossible to plot such data on a 2-d plane.\n",
        "\n",
        "\n",
        "Here are some of the benefits of applying dimensionality reduction to a dataset:\n",
        "\n",
        "* Space required to store the data is reduced as the number of dimensions comes down\n",
        "* Less dimensions lead to less computation/training time\n",
        "* Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful\n",
        "* It takes care of multicollinearity by removing redundant features. For example, you have two variables – ‘time spent on treadmill in minutes’ and ‘calories burnt’. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require\n",
        "* It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hstL9igFpzj4",
        "colab_type": "text"
      },
      "source": [
        "Dimensionality reduction can be done in two different ways:\n",
        "\n",
        "1. By only keeping the most relevant variables from the original dataset (this technique is called feature selection)\n",
        "2. By finding a smaller set of new variables, each being a combination of the input variables, containing basically the same information as the input variables (this technique is called dimensionality reduction)\n",
        "\n",
        "\n",
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/08/Screenshot-from-2018-08-10-12-07-43.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gOvRHzpqFDP",
        "colab_type": "text"
      },
      "source": [
        "**1)** **Missing Value Ratio**\n",
        "\n",
        "Suppose you’re given a dataset. What would be your first step? You would naturally want to explore the data first before building model. While exploring the data, you find that your dataset has some missing values. Now what? You will try to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods).\n",
        "\n",
        "What if we have too many missing values (say more than 50%)? Should we impute the missing values or drop the variable? We can set a threshold value and if the percentage of missing values in any variable is more than that threshold, we will drop the variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e_Jn6W_r32t",
        "colab_type": "text"
      },
      "source": [
        "**2) Low Variance Filter**\n",
        "\n",
        "Consider a variable in our dataset where all the observations have the same value, say 1. If we use this variable, do you think it can improve the model we will build? The answer is no, because this variable will have zero variance.\n",
        "\n",
        "So, we need to calculate the variance of each variable we are given. Then drop the variables having low variance as compared to other variables in our dataset. The reason is that variables with a low variance will not affect the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myRtPMiZb5rS",
        "colab_type": "text"
      },
      "source": [
        "**3) High Correlation filter**\n",
        "\n",
        "High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). We can calculate the correlation between independent numerical variables that are numerical in nature. If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind).\n",
        "\n",
        "As a general guideline, we should keep those variables which show a decent or high correlation with the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23VN5Rz0cCD4",
        "colab_type": "text"
      },
      "source": [
        "**4) Use of Random Forest**\n",
        "\n",
        "Random Forest is one of the most widely used algorithms for feature selection. It comes packaged with in-built feature importance so you don’t need to program that separately. This helps us select a smaller subset of features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i99Do4oxcaR1",
        "colab_type": "code",
        "outputId": "5f6d7f80-f199-458e-ab26-5de8f6d2f87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# a dummy code\n",
        "\n",
        "features = df.columns\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[-9:]  # top 10 features\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6b54f59b4ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# top 10 features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature Importances'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXYSHfpucgBn",
        "colab_type": "text"
      },
      "source": [
        "One will see something like this:\n",
        "\n",
        "![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/08/Screenshot-from-2018-07-26-23-28-54.png)\n",
        "\n",
        "\n",
        "\n",
        "Alernatively, we can use the SelectFromModel of sklearn to do so. It selects the features based on the importance of their weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahE2erXWcn0k",
        "colab_type": "code",
        "outputId": "db58a868-af15-439f-aa24-0addd9c508da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# dummy code\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "feature = SelectFromModel(model)\n",
        "Fit = feature.fit_transform(df, train.Item_Outlet_Sales)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-641f44f5c9de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mFit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItem_Outlet_Sales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HvU5dpqcsmI",
        "colab_type": "text"
      },
      "source": [
        "**5) Backward Feature Elimination**\n",
        "\n",
        "Follow the below steps to understand and use the ‘Backward Feature Elimination’ technique:\n",
        "\n",
        "* We first take all the n variables present in our dataset and train the model using them\n",
        "* We then calculate the performance of the model\n",
        "* Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables\n",
        "* We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n",
        "* Repeat this process until no variable can be dropped.\n",
        "\n",
        "\n",
        "This method can be used when building Linear Regression or Logistic Regression models. Let’s look at it’s Python implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTyZFKr4dAtU",
        "colab_type": "code",
        "outputId": "b17b620f-61b4-4750-8fa5-db96f6ace109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "# dummy code\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn import datasets\n",
        "lreg = LinearRegression()\n",
        "rfe = RFE(lreg, 10)\n",
        "rfe = rfe.fit_transform(df, train.Item_Outlet_Sales)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-059ab2ed25c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mItem_Outlet_Sales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gZn2tLVdMdE",
        "colab_type": "text"
      },
      "source": [
        "We need to specify the algorithm and number of features to select, and we get back the list of variables obtained from backward feature elimination. We can also check the ranking of the variables using the “rfe.ranking_” command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wxKrxWWdPKa",
        "colab_type": "text"
      },
      "source": [
        "**6) Forward Feature Selection**\n",
        "\n",
        "This is the opposite process of the Backward Feature Elimination we saw above. Instead of eliminating features, we try to find the best features which improve the performance of the model. This technique works as follows:\n",
        "\n",
        "*  We start with a single feature. Essentially, we train the model n number of times using each feature separately\n",
        "* The variable giving the best performance is selected as the starting variable\n",
        "* Then we repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained\n",
        "* We repeat this process until no significant improvement is seen in the model’s performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmeCWoeEdld9",
        "colab_type": "text"
      },
      "source": [
        "NOTE : Both Backward Feature Elimination and Forward Feature Selection are time consuming and computationally expensive.They are practically only used on datasets that have a small number of input variables.\n",
        "\n",
        "The techniques we have seen so far are generally used when we do not have a very large number of variables in our dataset. These are more or less feature selection techniques."
      ]
    }
  ]
}